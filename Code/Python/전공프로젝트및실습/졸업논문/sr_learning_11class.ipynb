{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 21 파일\n",
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 19 파일\n",
      "Foldername : 3 - 20 파일\n",
      "Foldername : 4 - 24 파일\n",
      "Foldername : 5 - 20 파일\n",
      "Foldername : 6 - 19 파일\n",
      "Foldername : 7 - 14 파일\n",
      "Foldername : 8 - 23 파일\n",
      "Foldername : 9 - 19 파일\n",
      "Foldername : data_10 - 19 파일\n",
      "X_data : (109843, 20)\n",
      "Y_label : (109843, 11)\n",
      "11 개의 클래스!!\n",
      "X_train : (82382, 20)\n",
      "Y_train : (82382, 11)\n",
      "X_test : (27461, 20)\n",
      "Y_test : (27461, 11)\n"
     ]
    }
   ],
   "source": [
    "#######################Tensorflow 코드 시작부분\n",
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "import os\n",
    "DATA_PATH = \"./data/\"\n",
    "X_train = []#train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "tf_classes = 0\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    " \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "#train 75% test 25%\n",
    "\n",
    " \n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_total :  (109843, 20)\n",
      "Y_total :  (109843, 11)\n"
     ]
    }
   ],
   "source": [
    "#total = trian + test set\n",
    "X_total = np.vstack([X_train, X_test])\n",
    "Y_total = np.vstack([Y_train, Y_test])\n",
    "print(\"X_total : \", np.shape(X_total))\n",
    "print(\"Y_total : \", np.shape(Y_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57667 samples, validate on 24715 samples\n",
      "Epoch 1/3000\n",
      "57667/57667 [==============================] - 9s 156us/step - loss: 2.0771 - accuracy: 0.4876 - val_loss: 0.8453 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84531, saving model to ./train_surv_time-0.8453.hdf5\n",
      "Epoch 2/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.8139 - accuracy: 0.7250 - val_loss: 0.5719 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84531 to 0.57194, saving model to ./train_surv_time-0.5719.hdf5\n",
      "Epoch 3/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.6151 - accuracy: 0.7882 - val_loss: 0.4762 - val_accuracy: 0.8387\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57194 to 0.47620, saving model to ./train_surv_time-0.4762.hdf5\n",
      "Epoch 4/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.5462 - accuracy: 0.8115 - val_loss: 0.4273 - val_accuracy: 0.8611\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47620 to 0.42730, saving model to ./train_surv_time-0.4273.hdf5\n",
      "Epoch 5/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.5079 - accuracy: 0.8240 - val_loss: 0.4160 - val_accuracy: 0.8666\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42730 to 0.41597, saving model to ./train_surv_time-0.4160.hdf5\n",
      "Epoch 6/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.4887 - accuracy: 0.8308 - val_loss: 0.4155 - val_accuracy: 0.8578\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.41597 to 0.41552, saving model to ./train_surv_time-0.4155.hdf5\n",
      "Epoch 7/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.4657 - accuracy: 0.8400 - val_loss: 0.3791 - val_accuracy: 0.8646\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.41552 to 0.37907, saving model to ./train_surv_time-0.3791.hdf5\n",
      "Epoch 8/3000\n",
      "57667/57667 [==============================] - 9s 147us/step - loss: 0.4549 - accuracy: 0.8430 - val_loss: 0.3817 - val_accuracy: 0.8668\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37907\n",
      "Epoch 9/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.4408 - accuracy: 0.8462 - val_loss: 0.3713 - val_accuracy: 0.8710\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.37907 to 0.37129, saving model to ./train_surv_time-0.3713.hdf5\n",
      "Epoch 10/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.4333 - accuracy: 0.8510 - val_loss: 0.3960 - val_accuracy: 0.8721\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37129\n",
      "Epoch 11/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.4227 - accuracy: 0.8547 - val_loss: 0.3738 - val_accuracy: 0.8692\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37129\n",
      "Epoch 12/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.4125 - accuracy: 0.8579 - val_loss: 0.3597 - val_accuracy: 0.8740\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.37129 to 0.35973, saving model to ./train_surv_time-0.3597.hdf5\n",
      "Epoch 13/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.4098 - accuracy: 0.8583 - val_loss: 0.3671 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35973\n",
      "Epoch 14/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.4006 - accuracy: 0.8606 - val_loss: 0.3509 - val_accuracy: 0.8812\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35973 to 0.35091, saving model to ./train_surv_time-0.3509.hdf5\n",
      "Epoch 15/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.4015 - accuracy: 0.8627 - val_loss: 0.3515 - val_accuracy: 0.8829\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35091\n",
      "Epoch 16/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.3988 - accuracy: 0.8622 - val_loss: 0.3409 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.35091 to 0.34085, saving model to ./train_surv_time-0.3409.hdf5\n",
      "Epoch 17/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.3907 - accuracy: 0.8657 - val_loss: 0.3531 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34085\n",
      "Epoch 18/3000\n",
      "57667/57667 [==============================] - 9s 147us/step - loss: 0.3830 - accuracy: 0.8681 - val_loss: 0.3444 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.34085\n",
      "Epoch 19/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.3839 - accuracy: 0.8687 - val_loss: 0.3193 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.34085 to 0.31929, saving model to ./train_surv_time-0.3193.hdf5\n",
      "Epoch 20/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.3815 - accuracy: 0.8696 - val_loss: 0.3175 - val_accuracy: 0.8899\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.31929 to 0.31749, saving model to ./train_surv_time-0.3175.hdf5\n",
      "Epoch 21/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.3739 - accuracy: 0.8716 - val_loss: 0.3558 - val_accuracy: 0.8742\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31749\n",
      "Epoch 22/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.3724 - accuracy: 0.8710 - val_loss: 0.3366 - val_accuracy: 0.8778\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31749\n",
      "Epoch 23/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.3654 - accuracy: 0.8749 - val_loss: 0.3346 - val_accuracy: 0.8819\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31749\n",
      "Epoch 24/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.3643 - accuracy: 0.8742 - val_loss: 0.3158 - val_accuracy: 0.8915\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.31749 to 0.31580, saving model to ./train_surv_time-0.3158.hdf5\n",
      "Epoch 25/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.3663 - accuracy: 0.8739 - val_loss: 0.3140 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.31580 to 0.31401, saving model to ./train_surv_time-0.3140.hdf5\n",
      "Epoch 26/3000\n",
      "57667/57667 [==============================] - 9s 156us/step - loss: 0.3608 - accuracy: 0.8759 - val_loss: 0.3281 - val_accuracy: 0.8897\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31401\n",
      "Epoch 27/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.3645 - accuracy: 0.8756 - val_loss: 0.3282 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.31401\n",
      "Epoch 28/3000\n",
      "57667/57667 [==============================] - 9s 149us/step - loss: 0.3627 - accuracy: 0.8760 - val_loss: 0.3291 - val_accuracy: 0.8883\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.31401\n",
      "Epoch 29/3000\n",
      "57667/57667 [==============================] - 9s 149us/step - loss: 0.3585 - accuracy: 0.8776 - val_loss: 0.3272 - val_accuracy: 0.8971\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.31401\n",
      "Epoch 30/3000\n",
      "57667/57667 [==============================] - 9s 157us/step - loss: 0.3522 - accuracy: 0.8792 - val_loss: 0.3188 - val_accuracy: 0.8903\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.31401\n",
      "Epoch 31/3000\n",
      "57667/57667 [==============================] - 9s 155us/step - loss: 0.3484 - accuracy: 0.8802 - val_loss: 0.3056 - val_accuracy: 0.8966\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.31401 to 0.30558, saving model to ./train_surv_time-0.3056.hdf5\n",
      "Epoch 32/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.3485 - accuracy: 0.8805 - val_loss: 0.3429 - val_accuracy: 0.8876\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.30558\n",
      "Epoch 33/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.3471 - accuracy: 0.8811 - val_loss: 0.3194 - val_accuracy: 0.8966\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.30558\n",
      "Epoch 34/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.3470 - accuracy: 0.8815 - val_loss: 0.3106 - val_accuracy: 0.8935\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.30558\n",
      "Epoch 35/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.3442 - accuracy: 0.8810 - val_loss: 0.3016 - val_accuracy: 0.8969\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.30558 to 0.30162, saving model to ./train_surv_time-0.3016.hdf5\n",
      "Epoch 36/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.3442 - accuracy: 0.8844 - val_loss: 0.3650 - val_accuracy: 0.8803\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.30162\n",
      "Epoch 37/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57667/57667 [==============================] - 8s 139us/step - loss: 0.3423 - accuracy: 0.8831 - val_loss: 0.3180 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.30162\n",
      "Epoch 38/3000\n",
      "57667/57667 [==============================] - 8s 139us/step - loss: 0.3372 - accuracy: 0.8854 - val_loss: 0.3209 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.30162\n",
      "Epoch 39/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.3397 - accuracy: 0.8832 - val_loss: 0.2953 - val_accuracy: 0.8985\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.30162 to 0.29529, saving model to ./train_surv_time-0.2953.hdf5\n",
      "Epoch 40/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3357 - accuracy: 0.8868 - val_loss: 0.3043 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.29529\n",
      "Epoch 41/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.3379 - accuracy: 0.8841 - val_loss: 0.3031 - val_accuracy: 0.9007\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.29529\n",
      "Epoch 42/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3332 - accuracy: 0.8854 - val_loss: 0.3071 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.29529\n",
      "Epoch 43/3000\n",
      "57667/57667 [==============================] - 9s 156us/step - loss: 0.3391 - accuracy: 0.8848 - val_loss: 0.3209 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.29529\n",
      "Epoch 44/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.3317 - accuracy: 0.8871 - val_loss: 0.2850 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.29529 to 0.28502, saving model to ./train_surv_time-0.2850.hdf5\n",
      "Epoch 45/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3283 - accuracy: 0.8894 - val_loss: 0.2828 - val_accuracy: 0.9060\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.28502 to 0.28280, saving model to ./train_surv_time-0.2828.hdf5\n",
      "Epoch 46/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.3206 - accuracy: 0.8913 - val_loss: 0.3289 - val_accuracy: 0.8899\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.28280\n",
      "Epoch 47/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3291 - accuracy: 0.8893 - val_loss: 0.2858 - val_accuracy: 0.9083\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.28280\n",
      "Epoch 48/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.3283 - accuracy: 0.8885 - val_loss: 0.3074 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.28280\n",
      "Epoch 49/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.3294 - accuracy: 0.8889 - val_loss: 0.2890 - val_accuracy: 0.9039\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.28280\n",
      "Epoch 50/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.3269 - accuracy: 0.8899 - val_loss: 0.3008 - val_accuracy: 0.8950\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.28280\n",
      "Epoch 51/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.3229 - accuracy: 0.8889 - val_loss: 0.3028 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.28280\n",
      "Epoch 52/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.3171 - accuracy: 0.8917 - val_loss: 0.2743 - val_accuracy: 0.9066\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.28280 to 0.27432, saving model to ./train_surv_time-0.2743.hdf5\n",
      "Epoch 53/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.3251 - accuracy: 0.8906 - val_loss: 0.2955 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.27432\n",
      "Epoch 54/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.3178 - accuracy: 0.8919 - val_loss: 0.3047 - val_accuracy: 0.8973\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.27432\n",
      "Epoch 55/3000\n",
      "57667/57667 [==============================] - 9s 149us/step - loss: 0.3291 - accuracy: 0.8899 - val_loss: 0.3059 - val_accuracy: 0.8964\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.27432\n",
      "Epoch 56/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.3241 - accuracy: 0.8912 - val_loss: 0.3372 - val_accuracy: 0.8897\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.27432\n",
      "Epoch 57/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.3188 - accuracy: 0.8919 - val_loss: 0.2902 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.27432\n",
      "Epoch 58/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.3118 - accuracy: 0.8945 - val_loss: 0.2931 - val_accuracy: 0.9039\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.27432\n",
      "Epoch 59/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.3123 - accuracy: 0.8929 - val_loss: 0.2887 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.27432\n",
      "Epoch 60/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.3145 - accuracy: 0.8937 - val_loss: 0.2859 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.27432\n",
      "Epoch 61/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.3070 - accuracy: 0.8951 - val_loss: 0.3378 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.27432\n",
      "Epoch 62/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3166 - accuracy: 0.8914 - val_loss: 0.2826 - val_accuracy: 0.9055\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.27432\n",
      "Epoch 63/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.3152 - accuracy: 0.8945 - val_loss: 0.2864 - val_accuracy: 0.9059\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.27432\n",
      "Epoch 64/3000\n",
      "57667/57667 [==============================] - 9s 158us/step - loss: 0.3148 - accuracy: 0.8940 - val_loss: 0.2915 - val_accuracy: 0.9049\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.27432\n",
      "Epoch 65/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.3092 - accuracy: 0.8948 - val_loss: 0.2656 - val_accuracy: 0.9145\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.27432 to 0.26564, saving model to ./train_surv_time-0.2656.hdf5\n",
      "Epoch 66/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3093 - accuracy: 0.8970 - val_loss: 0.2809 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.26564\n",
      "Epoch 67/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.3152 - accuracy: 0.8936 - val_loss: 0.2711 - val_accuracy: 0.9077\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.26564\n",
      "Epoch 68/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3191 - accuracy: 0.8929 - val_loss: 0.2828 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.26564\n",
      "Epoch 69/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.3051 - accuracy: 0.8985 - val_loss: 0.2746 - val_accuracy: 0.9081\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.26564\n",
      "Epoch 70/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.3071 - accuracy: 0.8962 - val_loss: 0.2886 - val_accuracy: 0.9002\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.26564\n",
      "Epoch 71/3000\n",
      "57667/57667 [==============================] - 9s 149us/step - loss: 0.3031 - accuracy: 0.8973 - val_loss: 0.2729 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.26564\n",
      "Epoch 72/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.3016 - accuracy: 0.8984 - val_loss: 0.2729 - val_accuracy: 0.9179\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.26564\n",
      "Epoch 73/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.3121 - accuracy: 0.8957 - val_loss: 0.2948 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.26564\n",
      "Epoch 74/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.3004 - accuracy: 0.8982 - val_loss: 0.2715 - val_accuracy: 0.9162\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.26564\n",
      "Epoch 75/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.3030 - accuracy: 0.8987 - val_loss: 0.2663 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.26564\n",
      "Epoch 76/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.3007 - accuracy: 0.8981 - val_loss: 0.2771 - val_accuracy: 0.9063\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.26564\n",
      "Epoch 77/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.2977 - accuracy: 0.8983 - val_loss: 0.2895 - val_accuracy: 0.9072\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.26564\n",
      "Epoch 78/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.3043 - accuracy: 0.8975 - val_loss: 0.2665 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.26564\n",
      "Epoch 79/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.3035 - accuracy: 0.8966 - val_loss: 0.2897 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.26564\n",
      "Epoch 80/3000\n",
      "57667/57667 [==============================] - 8s 138us/step - loss: 0.2977 - accuracy: 0.9001 - val_loss: 0.2622 - val_accuracy: 0.9116\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.26564 to 0.26222, saving model to ./train_surv_time-0.2622.hdf5\n",
      "Epoch 81/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2990 - accuracy: 0.8995 - val_loss: 0.2833 - val_accuracy: 0.9014\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.26222\n",
      "Epoch 82/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2960 - accuracy: 0.9012 - val_loss: 0.2800 - val_accuracy: 0.9120\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.26222\n",
      "Epoch 83/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2959 - accuracy: 0.9013 - val_loss: 0.2600 - val_accuracy: 0.9163\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.26222 to 0.26002, saving model to ./train_surv_time-0.2600.hdf5\n",
      "Epoch 84/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.2995 - accuracy: 0.8992 - val_loss: 0.2911 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.26002\n",
      "Epoch 85/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2998 - accuracy: 0.8977 - val_loss: 0.2690 - val_accuracy: 0.9137\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.26002\n",
      "Epoch 86/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2854 - accuracy: 0.9043 - val_loss: 0.3067 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.26002\n",
      "Epoch 87/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2942 - accuracy: 0.9005 - val_loss: 0.2716 - val_accuracy: 0.9138\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.26002\n",
      "Epoch 88/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2958 - accuracy: 0.9004 - val_loss: 0.2692 - val_accuracy: 0.9175\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.26002\n",
      "Epoch 89/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2954 - accuracy: 0.9008 - val_loss: 0.2702 - val_accuracy: 0.9137\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.26002\n",
      "Epoch 90/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2919 - accuracy: 0.9026 - val_loss: 0.2619 - val_accuracy: 0.9135\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.26002\n",
      "Epoch 91/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2890 - accuracy: 0.9030 - val_loss: 0.2705 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.26002\n",
      "Epoch 92/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2879 - accuracy: 0.9037 - val_loss: 0.2746 - val_accuracy: 0.9180\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.26002\n",
      "Epoch 93/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2903 - accuracy: 0.9032 - val_loss: 0.2533 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.26002 to 0.25327, saving model to ./train_surv_time-0.2533.hdf5\n",
      "Epoch 94/3000\n",
      "57667/57667 [==============================] - 9s 149us/step - loss: 0.2964 - accuracy: 0.9008 - val_loss: 0.2885 - val_accuracy: 0.9174\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.25327\n",
      "Epoch 95/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2908 - accuracy: 0.9026 - val_loss: 0.3061 - val_accuracy: 0.8971\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.25327\n",
      "Epoch 96/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2953 - accuracy: 0.9023 - val_loss: 0.2622 - val_accuracy: 0.9129\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.25327\n",
      "Epoch 97/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2821 - accuracy: 0.9041 - val_loss: 0.2939 - val_accuracy: 0.9039\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.25327\n",
      "Epoch 98/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2859 - accuracy: 0.9044 - val_loss: 0.2699 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.25327\n",
      "Epoch 99/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2820 - accuracy: 0.9058 - val_loss: 0.2739 - val_accuracy: 0.9070\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.25327\n",
      "Epoch 100/3000\n",
      "57667/57667 [==============================] - 11s 191us/step - loss: 0.2881 - accuracy: 0.9042 - val_loss: 0.2616 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.25327\n",
      "Epoch 101/3000\n",
      "57667/57667 [==============================] - 10s 167us/step - loss: 0.2888 - accuracy: 0.9039 - val_loss: 0.2584 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.25327\n",
      "Epoch 102/3000\n",
      "57667/57667 [==============================] - 10s 176us/step - loss: 0.2880 - accuracy: 0.9037 - val_loss: 0.2612 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.25327\n",
      "Epoch 103/3000\n",
      "57667/57667 [==============================] - 10s 180us/step - loss: 0.2951 - accuracy: 0.9029 - val_loss: 0.2766 - val_accuracy: 0.9109\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.25327\n",
      "Epoch 104/3000\n",
      "57667/57667 [==============================] - 10s 173us/step - loss: 0.2909 - accuracy: 0.9034 - val_loss: 0.2755 - val_accuracy: 0.9135\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.25327\n",
      "Epoch 105/3000\n",
      "57667/57667 [==============================] - 11s 198us/step - loss: 0.2815 - accuracy: 0.9050 - val_loss: 0.2762 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.25327\n",
      "Epoch 106/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2778 - accuracy: 0.9062 - val_loss: 0.2552 - val_accuracy: 0.9236\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.25327\n",
      "Epoch 107/3000\n",
      "57667/57667 [==============================] - 10s 177us/step - loss: 0.2781 - accuracy: 0.9070 - val_loss: 0.2421 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.25327 to 0.24209, saving model to ./train_surv_time-0.2421.hdf5\n",
      "Epoch 108/3000\n",
      "57667/57667 [==============================] - 10s 166us/step - loss: 0.2817 - accuracy: 0.9063 - val_loss: 0.2786 - val_accuracy: 0.9110\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24209\n",
      "Epoch 109/3000\n",
      "57667/57667 [==============================] - 10s 167us/step - loss: 0.2783 - accuracy: 0.9077 - val_loss: 0.2910 - val_accuracy: 0.9029\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24209\n",
      "Epoch 110/3000\n",
      "57667/57667 [==============================] - 10s 173us/step - loss: 0.2763 - accuracy: 0.9079 - val_loss: 0.2797 - val_accuracy: 0.9128\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.24209\n",
      "Epoch 111/3000\n",
      "57667/57667 [==============================] - 10s 175us/step - loss: 0.2826 - accuracy: 0.9066 - val_loss: 0.2943 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24209\n",
      "Epoch 112/3000\n",
      "57667/57667 [==============================] - 9s 162us/step - loss: 0.2801 - accuracy: 0.9068 - val_loss: 0.2624 - val_accuracy: 0.9158\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24209\n",
      "Epoch 113/3000\n",
      "57667/57667 [==============================] - 9s 162us/step - loss: 0.2772 - accuracy: 0.9081 - val_loss: 0.2828 - val_accuracy: 0.9172\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24209\n",
      "Epoch 114/3000\n",
      "57667/57667 [==============================] - 14s 244us/step - loss: 0.2830 - accuracy: 0.9066 - val_loss: 0.2459 - val_accuracy: 0.9171\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.24209\n",
      "Epoch 115/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57667/57667 [==============================] - 9s 159us/step - loss: 0.2772 - accuracy: 0.9075 - val_loss: 0.2705 - val_accuracy: 0.9162\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.24209\n",
      "Epoch 116/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2725 - accuracy: 0.9101 - val_loss: 0.2752 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.24209\n",
      "Epoch 117/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2694 - accuracy: 0.9109 - val_loss: 0.3276 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.24209\n",
      "Epoch 118/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2802 - accuracy: 0.9077 - val_loss: 0.2407 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.24209 to 0.24071, saving model to ./train_surv_time-0.2407.hdf5\n",
      "Epoch 119/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2740 - accuracy: 0.9088 - val_loss: 0.2319 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.24071 to 0.23192, saving model to ./train_surv_time-0.2319.hdf5\n",
      "Epoch 120/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2873 - accuracy: 0.9067 - val_loss: 0.2754 - val_accuracy: 0.9145\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.23192\n",
      "Epoch 121/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2691 - accuracy: 0.9095 - val_loss: 0.2266 - val_accuracy: 0.9229\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.23192 to 0.22659, saving model to ./train_surv_time-0.2266.hdf5\n",
      "Epoch 122/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2696 - accuracy: 0.9104 - val_loss: 0.2501 - val_accuracy: 0.9225\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.22659\n",
      "Epoch 123/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2844 - accuracy: 0.9074 - val_loss: 0.2650 - val_accuracy: 0.9155\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.22659\n",
      "Epoch 124/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2625 - accuracy: 0.9122 - val_loss: 0.2434 - val_accuracy: 0.9230\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.22659\n",
      "Epoch 125/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2674 - accuracy: 0.9113 - val_loss: 0.2489 - val_accuracy: 0.9300\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.22659\n",
      "Epoch 126/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2646 - accuracy: 0.9111 - val_loss: 0.2502 - val_accuracy: 0.9188\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.22659\n",
      "Epoch 127/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2612 - accuracy: 0.9122 - val_loss: 0.2643 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.22659\n",
      "Epoch 128/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2730 - accuracy: 0.9098 - val_loss: 0.2615 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.22659\n",
      "Epoch 129/3000\n",
      "57667/57667 [==============================] - 9s 155us/step - loss: 0.2714 - accuracy: 0.9097 - val_loss: 0.2259 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.22659 to 0.22594, saving model to ./train_surv_time-0.2259.hdf5\n",
      "Epoch 130/3000\n",
      "57667/57667 [==============================] - 9s 157us/step - loss: 0.2780 - accuracy: 0.9098 - val_loss: 0.2459 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.22594\n",
      "Epoch 131/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.2620 - accuracy: 0.9121 - val_loss: 0.2261 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.22594\n",
      "Epoch 132/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2623 - accuracy: 0.9131 - val_loss: 0.2551 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.22594\n",
      "Epoch 133/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2580 - accuracy: 0.9143 - val_loss: 0.2294 - val_accuracy: 0.9287\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.22594\n",
      "Epoch 134/3000\n",
      "57667/57667 [==============================] - 9s 155us/step - loss: 0.2795 - accuracy: 0.9100 - val_loss: 0.2382 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.22594\n",
      "Epoch 135/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2602 - accuracy: 0.9156 - val_loss: 0.2428 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.22594\n",
      "Epoch 136/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2601 - accuracy: 0.9155 - val_loss: 0.2451 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.22594\n",
      "Epoch 137/3000\n",
      "57667/57667 [==============================] - 9s 159us/step - loss: 0.2673 - accuracy: 0.9111 - val_loss: 0.2390 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.22594\n",
      "Epoch 138/3000\n",
      "57667/57667 [==============================] - 9s 153us/step - loss: 0.2651 - accuracy: 0.9124 - val_loss: 0.2231 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.22594 to 0.22310, saving model to ./train_surv_time-0.2231.hdf5\n",
      "Epoch 139/3000\n",
      "57667/57667 [==============================] - 9s 156us/step - loss: 0.2585 - accuracy: 0.9133 - val_loss: 0.2475 - val_accuracy: 0.9263\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.22310\n",
      "Epoch 140/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2595 - accuracy: 0.9139 - val_loss: 0.2454 - val_accuracy: 0.9256\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.22310\n",
      "Epoch 141/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2626 - accuracy: 0.9139 - val_loss: 0.2425 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.22310\n",
      "Epoch 142/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2681 - accuracy: 0.9126 - val_loss: 0.2578 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.22310\n",
      "Epoch 143/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2573 - accuracy: 0.9148 - val_loss: 0.2587 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.22310\n",
      "Epoch 144/3000\n",
      "57667/57667 [==============================] - 9s 153us/step - loss: 0.2636 - accuracy: 0.9129 - val_loss: 0.2829 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.22310\n",
      "Epoch 145/3000\n",
      "57667/57667 [==============================] - 9s 153us/step - loss: 0.2702 - accuracy: 0.9131 - val_loss: 0.2296 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.22310\n",
      "Epoch 146/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2563 - accuracy: 0.9148 - val_loss: 0.2542 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.22310\n",
      "Epoch 147/3000\n",
      "57667/57667 [==============================] - 9s 153us/step - loss: 0.2604 - accuracy: 0.9140 - val_loss: 0.2419 - val_accuracy: 0.9234\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.22310\n",
      "Epoch 148/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2481 - accuracy: 0.9201 - val_loss: 0.2267 - val_accuracy: 0.9327\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.22310\n",
      "Epoch 149/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2586 - accuracy: 0.9143 - val_loss: 0.2497 - val_accuracy: 0.9234\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.22310\n",
      "Epoch 150/3000\n",
      "57667/57667 [==============================] - 9s 157us/step - loss: 0.2596 - accuracy: 0.9149 - val_loss: 0.2158 - val_accuracy: 0.9325\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.22310 to 0.21580, saving model to ./train_surv_time-0.2158.hdf5\n",
      "Epoch 151/3000\n",
      "57667/57667 [==============================] - 9s 157us/step - loss: 0.2503 - accuracy: 0.9187 - val_loss: 0.2480 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.21580\n",
      "Epoch 152/3000\n",
      "57667/57667 [==============================] - 9s 155us/step - loss: 0.2444 - accuracy: 0.9186 - val_loss: 0.2284 - val_accuracy: 0.9339\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.21580\n",
      "Epoch 153/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2543 - accuracy: 0.9164 - val_loss: 0.2251 - val_accuracy: 0.9368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00153: val_loss did not improve from 0.21580\n",
      "Epoch 154/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2712 - accuracy: 0.9124 - val_loss: 0.2534 - val_accuracy: 0.9162\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.21580\n",
      "Epoch 155/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2503 - accuracy: 0.9185 - val_loss: 0.2262 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.21580\n",
      "Epoch 156/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2461 - accuracy: 0.9194 - val_loss: 0.2216 - val_accuracy: 0.9313\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.21580\n",
      "Epoch 157/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2467 - accuracy: 0.9188 - val_loss: 0.2466 - val_accuracy: 0.9295\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.21580\n",
      "Epoch 158/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.2485 - accuracy: 0.9185 - val_loss: 0.2176 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.21580\n",
      "Epoch 159/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2636 - accuracy: 0.9149 - val_loss: 0.2239 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.21580\n",
      "Epoch 160/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2558 - accuracy: 0.9161 - val_loss: 0.2269 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.21580\n",
      "Epoch 161/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2518 - accuracy: 0.9173 - val_loss: 0.2455 - val_accuracy: 0.9197\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.21580\n",
      "Epoch 162/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2497 - accuracy: 0.9193 - val_loss: 0.2122 - val_accuracy: 0.9332\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.21580 to 0.21222, saving model to ./train_surv_time-0.2122.hdf5\n",
      "Epoch 163/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.2519 - accuracy: 0.9169 - val_loss: 0.2248 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.21222\n",
      "Epoch 164/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2591 - accuracy: 0.9154 - val_loss: 0.2205 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.21222\n",
      "Epoch 165/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2478 - accuracy: 0.9189 - val_loss: 0.2162 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.21222\n",
      "Epoch 166/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2496 - accuracy: 0.9178 - val_loss: 0.2402 - val_accuracy: 0.9221\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.21222\n",
      "Epoch 167/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2451 - accuracy: 0.9202 - val_loss: 0.2105 - val_accuracy: 0.9360\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.21222 to 0.21053, saving model to ./train_surv_time-0.2105.hdf5\n",
      "Epoch 168/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2615 - accuracy: 0.9149 - val_loss: 0.2373 - val_accuracy: 0.9268\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.21053\n",
      "Epoch 169/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2476 - accuracy: 0.9189 - val_loss: 0.2258 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.21053\n",
      "Epoch 170/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2529 - accuracy: 0.9183 - val_loss: 0.2003 - val_accuracy: 0.9355\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.21053 to 0.20032, saving model to ./train_surv_time-0.2003.hdf5\n",
      "Epoch 171/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.2471 - accuracy: 0.9191 - val_loss: 0.2417 - val_accuracy: 0.9240\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.20032\n",
      "Epoch 172/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2380 - accuracy: 0.9211 - val_loss: 0.2261 - val_accuracy: 0.9304\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.20032\n",
      "Epoch 173/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2441 - accuracy: 0.9214 - val_loss: 0.2429 - val_accuracy: 0.9297\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.20032\n",
      "Epoch 174/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2488 - accuracy: 0.9188 - val_loss: 0.2239 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.20032\n",
      "Epoch 175/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2512 - accuracy: 0.9174 - val_loss: 0.2398 - val_accuracy: 0.9252\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.20032\n",
      "Epoch 176/3000\n",
      "57667/57667 [==============================] - 8s 145us/step - loss: 0.2446 - accuracy: 0.9201 - val_loss: 0.2276 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.20032\n",
      "Epoch 177/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2588 - accuracy: 0.9162 - val_loss: 0.2231 - val_accuracy: 0.9306\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.20032\n",
      "Epoch 178/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2504 - accuracy: 0.9196 - val_loss: 0.2410 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.20032\n",
      "Epoch 179/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2412 - accuracy: 0.9220 - val_loss: 0.2401 - val_accuracy: 0.9179\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.20032\n",
      "Epoch 180/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2485 - accuracy: 0.9199 - val_loss: 0.2317 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.20032\n",
      "Epoch 181/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.2463 - accuracy: 0.9183 - val_loss: 0.2467 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.20032\n",
      "Epoch 182/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2562 - accuracy: 0.9165 - val_loss: 0.2108 - val_accuracy: 0.9350\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.20032\n",
      "Epoch 183/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2382 - accuracy: 0.9218 - val_loss: 0.2263 - val_accuracy: 0.9283\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.20032\n",
      "Epoch 184/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2418 - accuracy: 0.9220 - val_loss: 0.2090 - val_accuracy: 0.9322\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.20032\n",
      "Epoch 185/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2400 - accuracy: 0.9220 - val_loss: 0.2375 - val_accuracy: 0.9326\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.20032\n",
      "Epoch 186/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2489 - accuracy: 0.9199 - val_loss: 0.1952 - val_accuracy: 0.9422\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.20032 to 0.19519, saving model to ./train_surv_time-0.1952.hdf5\n",
      "Epoch 187/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2433 - accuracy: 0.9218 - val_loss: 0.2194 - val_accuracy: 0.9323\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.19519\n",
      "Epoch 188/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2575 - accuracy: 0.9182 - val_loss: 0.2083 - val_accuracy: 0.9420\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.19519\n",
      "Epoch 189/3000\n",
      "57667/57667 [==============================] - 9s 153us/step - loss: 0.2474 - accuracy: 0.9193 - val_loss: 0.2205 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.19519\n",
      "Epoch 190/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2372 - accuracy: 0.9234 - val_loss: 0.2145 - val_accuracy: 0.9351\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.19519\n",
      "Epoch 191/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2474 - accuracy: 0.9198 - val_loss: 0.2626 - val_accuracy: 0.9178\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.19519\n",
      "Epoch 192/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2554 - accuracy: 0.9182 - val_loss: 0.2157 - val_accuracy: 0.9345\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.19519\n",
      "Epoch 193/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.2380 - accuracy: 0.9213 - val_loss: 0.2105 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.19519\n",
      "Epoch 194/3000\n",
      "57667/57667 [==============================] - 8s 136us/step - loss: 0.2403 - accuracy: 0.9210 - val_loss: 0.2802 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.19519\n",
      "Epoch 195/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2366 - accuracy: 0.9235 - val_loss: 0.2151 - val_accuracy: 0.9360\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.19519\n",
      "Epoch 196/3000\n",
      "57667/57667 [==============================] - 8s 136us/step - loss: 0.2497 - accuracy: 0.9204 - val_loss: 0.2502 - val_accuracy: 0.9277\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.19519\n",
      "Epoch 197/3000\n",
      "57667/57667 [==============================] - 10s 176us/step - loss: 0.2446 - accuracy: 0.9213 - val_loss: 0.2315 - val_accuracy: 0.9340\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.19519\n",
      "Epoch 198/3000\n",
      "57667/57667 [==============================] - 8s 141us/step - loss: 0.2502 - accuracy: 0.9190 - val_loss: 0.2199 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.19519\n",
      "Epoch 199/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2378 - accuracy: 0.9232 - val_loss: 0.2228 - val_accuracy: 0.9325\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.19519\n",
      "Epoch 200/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2337 - accuracy: 0.9240 - val_loss: 0.2290 - val_accuracy: 0.9342\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.19519\n",
      "Epoch 201/3000\n",
      "57667/57667 [==============================] - 8s 136us/step - loss: 0.2477 - accuracy: 0.9202 - val_loss: 0.2744 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.19519\n",
      "Epoch 202/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2446 - accuracy: 0.9218 - val_loss: 0.2051 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.19519\n",
      "Epoch 203/3000\n",
      "57667/57667 [==============================] - 9s 158us/step - loss: 0.2466 - accuracy: 0.9212 - val_loss: 0.2140 - val_accuracy: 0.9392\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.19519\n",
      "Epoch 204/3000\n",
      "57667/57667 [==============================] - 8s 139us/step - loss: 0.2430 - accuracy: 0.9217 - val_loss: 0.2004 - val_accuracy: 0.9353\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.19519\n",
      "Epoch 205/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2476 - accuracy: 0.9213 - val_loss: 0.2018 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.19519\n",
      "Epoch 206/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2378 - accuracy: 0.9231 - val_loss: 0.2043 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.19519\n",
      "Epoch 207/3000\n",
      "57667/57667 [==============================] - 8s 139us/step - loss: 0.2352 - accuracy: 0.9247 - val_loss: 0.2314 - val_accuracy: 0.9319\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.19519\n",
      "Epoch 208/3000\n",
      "57667/57667 [==============================] - 8s 144us/step - loss: 0.2455 - accuracy: 0.9224 - val_loss: 0.2136 - val_accuracy: 0.9313\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.19519\n",
      "Epoch 209/3000\n",
      "57667/57667 [==============================] - 8s 139us/step - loss: 0.2349 - accuracy: 0.9228 - val_loss: 0.2645 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.19519\n",
      "Epoch 210/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2441 - accuracy: 0.9215 - val_loss: 0.2151 - val_accuracy: 0.9322\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.19519\n",
      "Epoch 211/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.2379 - accuracy: 0.9230 - val_loss: 0.2203 - val_accuracy: 0.9347\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.19519\n",
      "Epoch 212/3000\n",
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2339 - accuracy: 0.9233 - val_loss: 0.2364 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.19519\n",
      "Epoch 213/3000\n",
      "57667/57667 [==============================] - 8s 140us/step - loss: 0.2419 - accuracy: 0.9222 - val_loss: 0.2248 - val_accuracy: 0.9334\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.19519\n",
      "Epoch 214/3000\n",
      "57667/57667 [==============================] - 8s 138us/step - loss: 0.2497 - accuracy: 0.9195 - val_loss: 0.2175 - val_accuracy: 0.9349\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.19519\n",
      "Epoch 215/3000\n",
      "57667/57667 [==============================] - 9s 150us/step - loss: 0.2433 - accuracy: 0.9222 - val_loss: 0.2142 - val_accuracy: 0.9383\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.19519\n",
      "Epoch 216/3000\n",
      "57667/57667 [==============================] - 10s 175us/step - loss: 0.2481 - accuracy: 0.9214 - val_loss: 0.2138 - val_accuracy: 0.9396\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.19519\n",
      "Epoch 217/3000\n",
      "57667/57667 [==============================] - 9s 153us/step - loss: 0.2296 - accuracy: 0.9249 - val_loss: 0.2407 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.19519\n",
      "Epoch 218/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.2376 - accuracy: 0.9231 - val_loss: 0.2304 - val_accuracy: 0.9298\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.19519\n",
      "Epoch 219/3000\n",
      "57667/57667 [==============================] - 9s 147us/step - loss: 0.2406 - accuracy: 0.9217 - val_loss: 0.2144 - val_accuracy: 0.9297\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.19519\n",
      "Epoch 220/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.2290 - accuracy: 0.9250 - val_loss: 0.2416 - val_accuracy: 0.9220\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.19519\n",
      "Epoch 221/3000\n",
      "57667/57667 [==============================] - 8s 142us/step - loss: 0.2408 - accuracy: 0.9216 - val_loss: 0.2323 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.19519\n",
      "Epoch 222/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2567 - accuracy: 0.9204 - val_loss: 0.2394 - val_accuracy: 0.9247\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.19519\n",
      "Epoch 223/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2373 - accuracy: 0.9237 - val_loss: 0.2594 - val_accuracy: 0.9173\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.19519\n",
      "Epoch 224/3000\n",
      "57667/57667 [==============================] - 8s 146us/step - loss: 0.2412 - accuracy: 0.9223 - val_loss: 0.2556 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.19519\n",
      "Epoch 225/3000\n",
      "57667/57667 [==============================] - 9s 152us/step - loss: 0.2383 - accuracy: 0.9232 - val_loss: 0.2129 - val_accuracy: 0.9331\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.19519\n",
      "Epoch 226/3000\n",
      "57667/57667 [==============================] - 9s 156us/step - loss: 0.2320 - accuracy: 0.9246 - val_loss: 0.2552 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.19519\n",
      "Epoch 227/3000\n",
      "57667/57667 [==============================] - 9s 154us/step - loss: 0.2422 - accuracy: 0.9224 - val_loss: 0.2393 - val_accuracy: 0.9306\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.19519\n",
      "Epoch 228/3000\n",
      "57667/57667 [==============================] - 9s 159us/step - loss: 0.2318 - accuracy: 0.9249 - val_loss: 0.2042 - val_accuracy: 0.9355\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.19519\n",
      "Epoch 229/3000\n",
      "57667/57667 [==============================] - 8s 147us/step - loss: 0.2385 - accuracy: 0.9244 - val_loss: 0.2478 - val_accuracy: 0.9265\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.19519\n",
      "Epoch 230/3000\n",
      "57667/57667 [==============================] - 9s 148us/step - loss: 0.2416 - accuracy: 0.9227 - val_loss: 0.2519 - val_accuracy: 0.9215\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.19519\n",
      "Epoch 231/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2424 - accuracy: 0.9220 - val_loss: 0.2895 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.19519\n",
      "Epoch 232/3000\n",
      "57667/57667 [==============================] - 9s 151us/step - loss: 0.2547 - accuracy: 0.9212 - val_loss: 0.2028 - val_accuracy: 0.9387\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.19519\n",
      "Epoch 233/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57667/57667 [==============================] - 8s 137us/step - loss: 0.2353 - accuracy: 0.9240 - val_loss: 0.2427 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.19519\n",
      "Epoch 234/3000\n",
      "57667/57667 [==============================] - 8s 138us/step - loss: 0.2411 - accuracy: 0.9232 - val_loss: 0.2252 - val_accuracy: 0.9381\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.19519\n",
      "Epoch 235/3000\n",
      "57667/57667 [==============================] - 8s 138us/step - loss: 0.2331 - accuracy: 0.9249 - val_loss: 0.2311 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.19519\n",
      "Epoch 236/3000\n",
      "57667/57667 [==============================] - 8s 143us/step - loss: 0.2352 - accuracy: 0.9251 - val_loss: 0.2148 - val_accuracy: 0.9374\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.19519\n",
      "\n",
      "elasped time =  0:33:47.929795\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "\n",
    "#keras\n",
    "seed = 0 \n",
    "np.random.seed(0)\n",
    "tf.compat.v1.set_random_seed(0)\n",
    "epochs, batch_size = 3000, 200 # batch -> 작을수록 좀 더 세밀한 조정 but 시간 많이걸림 \n",
    "#512,256,256 Act0.1 Dropout0.2\n",
    "start_time = datetime.datetime.now()\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = 20))#1st hidden layer\n",
    "model.add(LeakyReLU(0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))#2nd hidden layer  \n",
    "model.add(LeakyReLU(0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))#3rd hidden layer \n",
    "model.add(LeakyReLU(0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(tf_classes, activation = 'softmax'))#최종레이어\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "modelpath=\"./train_surv_time-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = model.fit(X_train, Y_train, validation_split=0.3, epochs=epochs, \n",
    "                    batch_size=batch_size, shuffle=True, callbacks=[early_stopping_callback,checkpointer])\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"\\nelasped time = \",end_time- start_time)\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test셋을 합쳐서 모델 초기화 후 learning -> epoch은 위에 참고해서 돌리기 , val_loss 기준 loss가 최소가 되었을때\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/294\n",
      "109843/109843 [==============================] - 13s 116us/step - loss: 0.2365 - accuracy: 0.9221\n",
      "Epoch 2/294\n",
      "  2400/109843 [..............................] - ETA: 8s - loss: 0.2566 - accuracy: 0.9204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\user\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2381 - accuracy: 0.9219\n",
      "Epoch 3/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2302 - accuracy: 0.9249\n",
      "Epoch 4/294\n",
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.2312 - accuracy: 0.9237\n",
      "Epoch 5/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2269 - accuracy: 0.9240\n",
      "Epoch 6/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2209 - accuracy: 0.9265\n",
      "Epoch 7/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2232 - accuracy: 0.9256\n",
      "Epoch 8/294\n",
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.2255 - accuracy: 0.9260\n",
      "Epoch 9/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2231 - accuracy: 0.9258\n",
      "Epoch 10/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2208 - accuracy: 0.9265\n",
      "Epoch 11/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2259 - accuracy: 0.9253\n",
      "Epoch 12/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2228 - accuracy: 0.9261\n",
      "Epoch 13/294\n",
      "109843/109843 [==============================] - 9s 78us/step - loss: 0.2166 - accuracy: 0.9281\n",
      "Epoch 14/294\n",
      "109843/109843 [==============================] - 9s 78us/step - loss: 0.2179 - accuracy: 0.9263\n",
      "Epoch 15/294\n",
      "109843/109843 [==============================] - 9s 78us/step - loss: 0.2179 - accuracy: 0.9272\n",
      "Epoch 16/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2267 - accuracy: 0.9245\n",
      "Epoch 17/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2167 - accuracy: 0.9286\n",
      "Epoch 18/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.2168 - accuracy: 0.9280\n",
      "Epoch 19/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2163 - accuracy: 0.9284\n",
      "Epoch 20/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2204 - accuracy: 0.9266\n",
      "Epoch 21/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2173 - accuracy: 0.9278\n",
      "Epoch 22/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2168 - accuracy: 0.9282\n",
      "Epoch 23/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2141 - accuracy: 0.9300\n",
      "Epoch 24/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.2134 - accuracy: 0.9280\n",
      "Epoch 25/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2119 - accuracy: 0.9295\n",
      "Epoch 26/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2185 - accuracy: 0.9281\n",
      "Epoch 27/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2141 - accuracy: 0.9282\n",
      "Epoch 28/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2180 - accuracy: 0.9273\n",
      "Epoch 29/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2212 - accuracy: 0.9282\n",
      "Epoch 30/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2127 - accuracy: 0.9290\n",
      "Epoch 31/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2151 - accuracy: 0.9280\n",
      "Epoch 32/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2152 - accuracy: 0.9292\n",
      "Epoch 33/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.2112 - accuracy: 0.9295\n",
      "Epoch 34/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2239 - accuracy: 0.9272\n",
      "Epoch 35/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2058 - accuracy: 0.9311\n",
      "Epoch 36/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2064 - accuracy: 0.9310\n",
      "Epoch 37/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2101 - accuracy: 0.9298\n",
      "Epoch 38/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2117 - accuracy: 0.9300\n",
      "Epoch 39/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2067 - accuracy: 0.9311\n",
      "Epoch 40/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2108 - accuracy: 0.9296\n",
      "Epoch 41/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2105 - accuracy: 0.9297\n",
      "Epoch 42/294\n",
      "109843/109843 [==============================] - 11s 97us/step - loss: 0.2172 - accuracy: 0.9282\n",
      "Epoch 43/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.2176 - accuracy: 0.9283\n",
      "Epoch 44/294\n",
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.2013 - accuracy: 0.9324\n",
      "Epoch 45/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2099 - accuracy: 0.9296\n",
      "Epoch 46/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2080 - accuracy: 0.9315\n",
      "Epoch 47/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2067 - accuracy: 0.9319\n",
      "Epoch 48/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2126 - accuracy: 0.9301\n",
      "Epoch 49/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.2128 - accuracy: 0.9298\n",
      "Epoch 50/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2091 - accuracy: 0.9302\n",
      "Epoch 51/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2076 - accuracy: 0.9309\n",
      "Epoch 52/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2120 - accuracy: 0.9303\n",
      "Epoch 53/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2082 - accuracy: 0.9312\n",
      "Epoch 54/294\n",
      "109843/109843 [==============================] - 11s 104us/step - loss: 0.2050 - accuracy: 0.9313\n",
      "Epoch 55/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.2099 - accuracy: 0.9309\n",
      "Epoch 56/294\n",
      "109843/109843 [==============================] - 10s 90us/step - loss: 0.2114 - accuracy: 0.9300\n",
      "Epoch 57/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2082 - accuracy: 0.9306\n",
      "Epoch 58/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2073 - accuracy: 0.9314\n",
      "Epoch 59/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2035 - accuracy: 0.9334\n",
      "Epoch 60/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2094 - accuracy: 0.9309\n",
      "Epoch 61/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2079 - accuracy: 0.9318\n",
      "Epoch 62/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2038 - accuracy: 0.9319\n",
      "Epoch 63/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.2111 - accuracy: 0.9305 0s -\n",
      "Epoch 64/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.2083 - accuracy: 0.9315\n",
      "Epoch 65/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2046 - accuracy: 0.9318\n",
      "Epoch 66/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2088 - accuracy: 0.9308\n",
      "Epoch 67/294\n",
      "109843/109843 [==============================] - 9s 79us/step - loss: 0.2006 - accuracy: 0.9332\n",
      "Epoch 68/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2078 - accuracy: 0.9310\n",
      "Epoch 69/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2068 - accuracy: 0.9321\n",
      "Epoch 70/294\n",
      "109843/109843 [==============================] - 10s 91us/step - loss: 0.2059 - accuracy: 0.9314\n",
      "Epoch 71/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2059 - accuracy: 0.9324\n",
      "Epoch 72/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2022 - accuracy: 0.9323\n",
      "Epoch 73/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.2023 - accuracy: 0.9329\n",
      "Epoch 74/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2053 - accuracy: 0.9323\n",
      "Epoch 75/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2079 - accuracy: 0.9322\n",
      "Epoch 76/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.2054 - accuracy: 0.9308\n",
      "Epoch 77/294\n",
      "109843/109843 [==============================] - 10s 89us/step - loss: 0.2073 - accuracy: 0.9321\n",
      "Epoch 78/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2023 - accuracy: 0.9326\n",
      "Epoch 79/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2125 - accuracy: 0.9300\n",
      "Epoch 80/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.2025 - accuracy: 0.9333\n",
      "Epoch 81/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2043 - accuracy: 0.9327\n",
      "Epoch 82/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2068 - accuracy: 0.9322\n",
      "Epoch 83/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2041 - accuracy: 0.9321\n",
      "Epoch 84/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2011 - accuracy: 0.9335\n",
      "Epoch 85/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2055 - accuracy: 0.9321\n",
      "Epoch 86/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2040 - accuracy: 0.9329\n",
      "Epoch 87/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2025 - accuracy: 0.9329\n",
      "Epoch 88/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2007 - accuracy: 0.9344\n",
      "Epoch 89/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.1980 - accuracy: 0.9340\n",
      "Epoch 90/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.2004 - accuracy: 0.9335\n",
      "Epoch 91/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2015 - accuracy: 0.9331\n",
      "Epoch 92/294\n",
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.2044 - accuracy: 0.9323\n",
      "Epoch 93/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2127 - accuracy: 0.9310\n",
      "Epoch 94/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1973 - accuracy: 0.9349\n",
      "Epoch 95/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2073 - accuracy: 0.9318\n",
      "Epoch 96/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2058 - accuracy: 0.9332\n",
      "Epoch 97/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1931 - accuracy: 0.9356\n",
      "Epoch 98/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.2024 - accuracy: 0.9329\n",
      "Epoch 99/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.2003 - accuracy: 0.9343\n",
      "Epoch 100/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.1952 - accuracy: 0.9350\n",
      "Epoch 101/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1981 - accuracy: 0.9344\n",
      "Epoch 102/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2006 - accuracy: 0.9345\n",
      "Epoch 103/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.2005 - accuracy: 0.9340\n",
      "Epoch 104/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2062 - accuracy: 0.9323\n",
      "Epoch 105/294\n",
      "109843/109843 [==============================] - 11s 103us/step - loss: 0.2094 - accuracy: 0.9327\n",
      "Epoch 106/294\n",
      "109843/109843 [==============================] - 10s 90us/step - loss: 0.2036 - accuracy: 0.9327\n",
      "Epoch 107/294\n",
      "109843/109843 [==============================] - 11s 101us/step - loss: 0.1964 - accuracy: 0.9349\n",
      "Epoch 108/294\n",
      "109843/109843 [==============================] - 10s 91us/step - loss: 0.2032 - accuracy: 0.9329\n",
      "Epoch 109/294\n",
      "109843/109843 [==============================] - 12s 110us/step - loss: 0.2010 - accuracy: 0.9338\n",
      "Epoch 110/294\n",
      "109843/109843 [==============================] - 11s 97us/step - loss: 0.1959 - accuracy: 0.9351\n",
      "Epoch 111/294\n",
      "109843/109843 [==============================] - 11s 100us/step - loss: 0.2027 - accuracy: 0.9343\n",
      "Epoch 112/294\n",
      "109843/109843 [==============================] - 11s 97us/step - loss: 0.1969 - accuracy: 0.9355\n",
      "Epoch 113/294\n",
      "109843/109843 [==============================] - 11s 99us/step - loss: 0.2031 - accuracy: 0.9332\n",
      "Epoch 114/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2027 - accuracy: 0.9335\n",
      "Epoch 115/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.1985 - accuracy: 0.9346\n",
      "Epoch 116/294\n",
      "109843/109843 [==============================] - 10s 91us/step - loss: 0.2007 - accuracy: 0.9336\n",
      "Epoch 117/294\n",
      "109843/109843 [==============================] - 14s 128us/step - loss: 0.1999 - accuracy: 0.9336\n",
      "Epoch 118/294\n",
      "109843/109843 [==============================] - 13s 114us/step - loss: 0.1959 - accuracy: 0.9349\n",
      "Epoch 119/294\n",
      "109843/109843 [==============================] - 10s 90us/step - loss: 0.2007 - accuracy: 0.9337\n",
      "Epoch 120/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.2026 - accuracy: 0.9342\n",
      "Epoch 121/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.1944 - accuracy: 0.9355\n",
      "Epoch 122/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.2041 - accuracy: 0.9339\n",
      "Epoch 123/294\n",
      "109843/109843 [==============================] - 11s 102us/step - loss: 0.1992 - accuracy: 0.9342\n",
      "Epoch 124/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.1950 - accuracy: 0.9362\n",
      "Epoch 125/294\n",
      "109843/109843 [==============================] - 10s 89us/step - loss: 0.2022 - accuracy: 0.9335\n",
      "Epoch 126/294\n",
      "109843/109843 [==============================] - 11s 96us/step - loss: 0.2033 - accuracy: 0.9337\n",
      "Epoch 127/294\n",
      "109843/109843 [==============================] - 13s 116us/step - loss: 0.1935 - accuracy: 0.9357\n",
      "Epoch 128/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1969 - accuracy: 0.9350\n",
      "Epoch 129/294\n",
      "109843/109843 [==============================] - 11s 104us/step - loss: 0.2008 - accuracy: 0.9338\n",
      "Epoch 130/294\n",
      "109843/109843 [==============================] - 10s 92us/step - loss: 0.2001 - accuracy: 0.9347\n",
      "Epoch 131/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.2060 - accuracy: 0.9331\n",
      "Epoch 132/294\n",
      "109843/109843 [==============================] - 11s 96us/step - loss: 0.1967 - accuracy: 0.9342\n",
      "Epoch 133/294\n",
      "109843/109843 [==============================] - 10s 95us/step - loss: 0.1961 - accuracy: 0.9357\n",
      "Epoch 134/294\n",
      "109843/109843 [==============================] - 11s 98us/step - loss: 0.2016 - accuracy: 0.9343\n",
      "Epoch 135/294\n",
      "109843/109843 [==============================] - 11s 98us/step - loss: 0.1949 - accuracy: 0.9361\n",
      "Epoch 136/294\n",
      "109843/109843 [==============================] - 11s 101us/step - loss: 0.2030 - accuracy: 0.9345\n",
      "Epoch 137/294\n",
      "109843/109843 [==============================] - 11s 99us/step - loss: 0.1898 - accuracy: 0.9375\n",
      "Epoch 138/294\n",
      "109843/109843 [==============================] - 10s 92us/step - loss: 0.1997 - accuracy: 0.9341\n",
      "Epoch 139/294\n",
      "109843/109843 [==============================] - 13s 119us/step - loss: 0.1986 - accuracy: 0.9343\n",
      "Epoch 140/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.1957 - accuracy: 0.93611s\n",
      "Epoch 141/294\n",
      "109843/109843 [==============================] - 10s 88us/step - loss: 0.2031 - accuracy: 0.9341\n",
      "Epoch 142/294\n",
      "109843/109843 [==============================] - 11s 99us/step - loss: 0.1956 - accuracy: 0.9357\n",
      "Epoch 143/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1938 - accuracy: 0.9364\n",
      "Epoch 144/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1984 - accuracy: 0.9347\n",
      "Epoch 145/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1948 - accuracy: 0.9370\n",
      "Epoch 146/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1939 - accuracy: 0.9364\n",
      "Epoch 147/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.2027 - accuracy: 0.9338\n",
      "Epoch 148/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1940 - accuracy: 0.9358\n",
      "Epoch 149/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.1993 - accuracy: 0.9347\n",
      "Epoch 150/294\n",
      "109843/109843 [==============================] - 10s 89us/step - loss: 0.1961 - accuracy: 0.9352\n",
      "Epoch 151/294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.1975 - accuracy: 0.9352\n",
      "Epoch 152/294\n",
      "109843/109843 [==============================] - 12s 105us/step - loss: 0.1920 - accuracy: 0.9359\n",
      "Epoch 153/294\n",
      "109843/109843 [==============================] - 13s 118us/step - loss: 0.2071 - accuracy: 0.9344\n",
      "Epoch 154/294\n",
      "109843/109843 [==============================] - 17s 155us/step - loss: 0.1936 - accuracy: 0.9368\n",
      "Epoch 155/294\n",
      "109843/109843 [==============================] - 11s 104us/step - loss: 0.2002 - accuracy: 0.9350\n",
      "Epoch 156/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1999 - accuracy: 0.9347\n",
      "Epoch 157/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.2058 - accuracy: 0.9343\n",
      "Epoch 158/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1951 - accuracy: 0.9364\n",
      "Epoch 159/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1990 - accuracy: 0.9344\n",
      "Epoch 160/294\n",
      "109843/109843 [==============================] - 12s 114us/step - loss: 0.1936 - accuracy: 0.9363\n",
      "Epoch 161/294\n",
      "109843/109843 [==============================] - 13s 118us/step - loss: 0.1935 - accuracy: 0.9360\n",
      "Epoch 162/294\n",
      "109843/109843 [==============================] - 12s 109us/step - loss: 0.1950 - accuracy: 0.9360\n",
      "Epoch 163/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1922 - accuracy: 0.9371\n",
      "Epoch 164/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1961 - accuracy: 0.9361\n",
      "Epoch 165/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1953 - accuracy: 0.9359\n",
      "Epoch 166/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.1922 - accuracy: 0.9363\n",
      "Epoch 167/294\n",
      "109843/109843 [==============================] - 11s 97us/step - loss: 0.1998 - accuracy: 0.9352\n",
      "Epoch 168/294\n",
      "109843/109843 [==============================] - 20s 179us/step - loss: 0.2002 - accuracy: 0.9342\n",
      "Epoch 169/294\n",
      "109843/109843 [==============================] - 12s 109us/step - loss: 0.1937 - accuracy: 0.9358\n",
      "Epoch 170/294\n",
      "109843/109843 [==============================] - 11s 96us/step - loss: 0.1977 - accuracy: 0.9358\n",
      "Epoch 171/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.2010 - accuracy: 0.9349\n",
      "Epoch 172/294\n",
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.1916 - accuracy: 0.9370\n",
      "Epoch 173/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.1939 - accuracy: 0.9364\n",
      "Epoch 174/294\n",
      "109843/109843 [==============================] - 10s 87us/step - loss: 0.1936 - accuracy: 0.9364\n",
      "Epoch 175/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1925 - accuracy: 0.9364\n",
      "Epoch 176/294\n",
      "109843/109843 [==============================] - 10s 90us/step - loss: 0.1939 - accuracy: 0.9362\n",
      "Epoch 177/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.1947 - accuracy: 0.9366\n",
      "Epoch 178/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1979 - accuracy: 0.9350\n",
      "Epoch 179/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1995 - accuracy: 0.9347\n",
      "Epoch 180/294\n",
      "109843/109843 [==============================] - 8s 74us/step - loss: 0.1915 - accuracy: 0.9362\n",
      "Epoch 181/294\n",
      "109843/109843 [==============================] - 8s 73us/step - loss: 0.1939 - accuracy: 0.9371\n",
      "Epoch 182/294\n",
      "109843/109843 [==============================] - 8s 73us/step - loss: 0.1949 - accuracy: 0.9362\n",
      "Epoch 183/294\n",
      "109843/109843 [==============================] - 8s 73us/step - loss: 0.1918 - accuracy: 0.9368\n",
      "Epoch 184/294\n",
      "109843/109843 [==============================] - 8s 73us/step - loss: 0.1976 - accuracy: 0.9356\n",
      "Epoch 185/294\n",
      "109843/109843 [==============================] - 12s 106us/step - loss: 0.1963 - accuracy: 0.9352\n",
      "Epoch 186/294\n",
      "109843/109843 [==============================] - 13s 114us/step - loss: 0.1975 - accuracy: 0.9366\n",
      "Epoch 187/294\n",
      "109843/109843 [==============================] - 13s 116us/step - loss: 0.1917 - accuracy: 0.9369\n",
      "Epoch 188/294\n",
      "109843/109843 [==============================] - 13s 120us/step - loss: 0.1912 - accuracy: 0.9366\n",
      "Epoch 189/294\n",
      "109843/109843 [==============================] - 13s 120us/step - loss: 0.1997 - accuracy: 0.9358\n",
      "Epoch 190/294\n",
      "109843/109843 [==============================] - 23s 210us/step - loss: 0.1952 - accuracy: 0.9372\n",
      "Epoch 191/294\n",
      "109843/109843 [==============================] - 15s 139us/step - loss: 0.1913 - accuracy: 0.9379\n",
      "Epoch 192/294\n",
      "109843/109843 [==============================] - 15s 138us/step - loss: 0.1967 - accuracy: 0.9357\n",
      "Epoch 193/294\n",
      "109843/109843 [==============================] - 14s 129us/step - loss: 0.1892 - accuracy: 0.9378\n",
      "Epoch 194/294\n",
      "109843/109843 [==============================] - 14s 127us/step - loss: 0.1969 - accuracy: 0.9367\n",
      "Epoch 195/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1940 - accuracy: 0.9370\n",
      "Epoch 196/294\n",
      "109843/109843 [==============================] - 15s 136us/step - loss: 0.1990 - accuracy: 0.9357\n",
      "Epoch 197/294\n",
      "109843/109843 [==============================] - 18s 161us/step - loss: 0.1918 - accuracy: 0.9367\n",
      "Epoch 198/294\n",
      "109843/109843 [==============================] - 14s 132us/step - loss: 0.1963 - accuracy: 0.9353\n",
      "Epoch 199/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1929 - accuracy: 0.9374\n",
      "Epoch 200/294\n",
      "109843/109843 [==============================] - 15s 137us/step - loss: 0.1951 - accuracy: 0.9370\n",
      "Epoch 201/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1939 - accuracy: 0.9366\n",
      "Epoch 202/294\n",
      "109843/109843 [==============================] - 13s 115us/step - loss: 0.1924 - accuracy: 0.9370\n",
      "Epoch 203/294\n",
      "109843/109843 [==============================] - 11s 97us/step - loss: 0.1944 - accuracy: 0.9364\n",
      "Epoch 204/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1964 - accuracy: 0.9362\n",
      "Epoch 205/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1965 - accuracy: 0.9362\n",
      "Epoch 206/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1925 - accuracy: 0.9372\n",
      "Epoch 207/294\n",
      "109843/109843 [==============================] - 9s 85us/step - loss: 0.1893 - accuracy: 0.9378\n",
      "Epoch 208/294\n",
      "109843/109843 [==============================] - 12s 111us/step - loss: 0.1908 - accuracy: 0.9381\n",
      "Epoch 209/294\n",
      "109843/109843 [==============================] - 10s 93us/step - loss: 0.1920 - accuracy: 0.9364\n",
      "Epoch 210/294\n",
      "109843/109843 [==============================] - 14s 126us/step - loss: 0.1891 - accuracy: 0.9379\n",
      "Epoch 211/294\n",
      "109843/109843 [==============================] - 14s 124us/step - loss: 0.1913 - accuracy: 0.9376\n",
      "Epoch 212/294\n",
      "109843/109843 [==============================] - 15s 141us/step - loss: 0.1940 - accuracy: 0.9370\n",
      "Epoch 213/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1994 - accuracy: 0.9353\n",
      "Epoch 214/294\n",
      "109843/109843 [==============================] - 15s 134us/step - loss: 0.1902 - accuracy: 0.9377\n",
      "Epoch 215/294\n",
      "109843/109843 [==============================] - 15s 141us/step - loss: 0.1960 - accuracy: 0.9366\n",
      "Epoch 216/294\n",
      "109843/109843 [==============================] - 15s 137us/step - loss: 0.1906 - accuracy: 0.9377\n",
      "Epoch 217/294\n",
      "109843/109843 [==============================] - 15s 138us/step - loss: 0.1952 - accuracy: 0.9365\n",
      "Epoch 218/294\n",
      "109843/109843 [==============================] - 15s 136us/step - loss: 0.1933 - accuracy: 0.9366\n",
      "Epoch 219/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1900 - accuracy: 0.9376\n",
      "Epoch 220/294\n",
      "109843/109843 [==============================] - 15s 135us/step - loss: 0.1953 - accuracy: 0.9375\n",
      "Epoch 221/294\n",
      "109843/109843 [==============================] - 15s 135us/step - loss: 0.1899 - accuracy: 0.9381\n",
      "Epoch 222/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1905 - accuracy: 0.9379\n",
      "Epoch 223/294\n",
      "109843/109843 [==============================] - 15s 137us/step - loss: 0.1964 - accuracy: 0.9366\n",
      "Epoch 224/294\n",
      "109843/109843 [==============================] - 13s 123us/step - loss: 0.1920 - accuracy: 0.9376\n",
      "Epoch 225/294\n",
      "109843/109843 [==============================] - 13s 122us/step - loss: 0.1881 - accuracy: 0.9386\n",
      "Epoch 226/294\n",
      "109843/109843 [==============================] - 14s 129us/step - loss: 0.1933 - accuracy: 0.9368\n",
      "Epoch 227/294\n",
      "109843/109843 [==============================] - 14s 128us/step - loss: 0.1959 - accuracy: 0.9365\n",
      "Epoch 228/294\n",
      "109843/109843 [==============================] - 16s 143us/step - loss: 0.1867 - accuracy: 0.9390\n",
      "Epoch 229/294\n",
      "109843/109843 [==============================] - 15s 132us/step - loss: 0.1906 - accuracy: 0.9371\n",
      "Epoch 230/294\n",
      "109843/109843 [==============================] - 14s 129us/step - loss: 0.1975 - accuracy: 0.9363\n",
      "Epoch 231/294\n",
      "109843/109843 [==============================] - 14s 126us/step - loss: 0.1933 - accuracy: 0.9373\n",
      "Epoch 232/294\n",
      "109843/109843 [==============================] - 14s 129us/step - loss: 0.1941 - accuracy: 0.9368\n",
      "Epoch 233/294\n",
      "109843/109843 [==============================] - 14s 132us/step - loss: 0.1942 - accuracy: 0.9377\n",
      "Epoch 234/294\n",
      "109843/109843 [==============================] - 15s 135us/step - loss: 0.1894 - accuracy: 0.9384\n",
      "Epoch 235/294\n",
      "109843/109843 [==============================] - 15s 135us/step - loss: 0.1990 - accuracy: 0.9352\n",
      "Epoch 236/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1966 - accuracy: 0.9363\n",
      "Epoch 237/294\n",
      "109843/109843 [==============================] - 14s 130us/step - loss: 0.1897 - accuracy: 0.9387\n",
      "Epoch 238/294\n",
      "109843/109843 [==============================] - 16s 146us/step - loss: 0.1899 - accuracy: 0.9384\n",
      "Epoch 239/294\n",
      "109843/109843 [==============================] - 15s 137us/step - loss: 0.1880 - accuracy: 0.9386\n",
      "Epoch 240/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1950 - accuracy: 0.9365\n",
      "Epoch 241/294\n",
      "109843/109843 [==============================] - 14s 127us/step - loss: 0.1884 - accuracy: 0.9392\n",
      "Epoch 242/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1957 - accuracy: 0.9368\n",
      "Epoch 243/294\n",
      "109843/109843 [==============================] - 16s 142us/step - loss: 0.1939 - accuracy: 0.9379\n",
      "Epoch 244/294\n",
      "109843/109843 [==============================] - 16s 142us/step - loss: 0.1922 - accuracy: 0.9373\n",
      "Epoch 245/294\n",
      "109843/109843 [==============================] - 16s 145us/step - loss: 0.1915 - accuracy: 0.9367\n",
      "Epoch 246/294\n",
      "109843/109843 [==============================] - 17s 156us/step - loss: 0.1894 - accuracy: 0.9368\n",
      "Epoch 247/294\n",
      "109843/109843 [==============================] - 18s 164us/step - loss: 0.1885 - accuracy: 0.9382\n",
      "Epoch 248/294\n",
      "109843/109843 [==============================] - 15s 134us/step - loss: 0.1886 - accuracy: 0.9377\n",
      "Epoch 249/294\n",
      "109843/109843 [==============================] - 14s 130us/step - loss: 0.1954 - accuracy: 0.9367\n",
      "Epoch 250/294\n",
      "109843/109843 [==============================] - 9s 86us/step - loss: 0.1932 - accuracy: 0.9366\n",
      "Epoch 251/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1888 - accuracy: 0.9382\n",
      "Epoch 252/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1894 - accuracy: 0.9373\n",
      "Epoch 253/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1923 - accuracy: 0.9384\n",
      "Epoch 254/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1907 - accuracy: 0.9386\n",
      "Epoch 255/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1905 - accuracy: 0.9371\n",
      "Epoch 256/294\n",
      "109843/109843 [==============================] - 9s 84us/step - loss: 0.1853 - accuracy: 0.9391\n",
      "Epoch 257/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1911 - accuracy: 0.9377\n",
      "Epoch 258/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.1863 - accuracy: 0.9383\n",
      "Epoch 259/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1976 - accuracy: 0.9360\n",
      "Epoch 260/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1981 - accuracy: 0.9368\n",
      "Epoch 261/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1947 - accuracy: 0.9375\n",
      "Epoch 262/294\n",
      "109843/109843 [==============================] - 9s 80us/step - loss: 0.1938 - accuracy: 0.9371\n",
      "Epoch 263/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1895 - accuracy: 0.9385\n",
      "Epoch 264/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1869 - accuracy: 0.9396\n",
      "Epoch 265/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1888 - accuracy: 0.9382\n",
      "Epoch 266/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1956 - accuracy: 0.9371\n",
      "Epoch 267/294\n",
      "109843/109843 [==============================] - 9s 83us/step - loss: 0.1943 - accuracy: 0.9370\n",
      "Epoch 268/294\n",
      "109843/109843 [==============================] - 9s 81us/step - loss: 0.1929 - accuracy: 0.9376\n",
      "Epoch 269/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1914 - accuracy: 0.9378\n",
      "Epoch 270/294\n",
      "109843/109843 [==============================] - 9s 82us/step - loss: 0.1887 - accuracy: 0.9388\n",
      "Epoch 271/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1909 - accuracy: 0.9381\n",
      "Epoch 272/294\n",
      "109843/109843 [==============================] - 14s 126us/step - loss: 0.1938 - accuracy: 0.9374\n",
      "Epoch 273/294\n",
      "109843/109843 [==============================] - 14s 129us/step - loss: 0.1837 - accuracy: 0.9397\n",
      "Epoch 274/294\n",
      "109843/109843 [==============================] - 15s 137us/step - loss: 0.1891 - accuracy: 0.9397\n",
      "Epoch 275/294\n",
      "109843/109843 [==============================] - 15s 139us/step - loss: 0.1940 - accuracy: 0.9378\n",
      "Epoch 276/294\n",
      "109843/109843 [==============================] - 14s 126us/step - loss: 0.1940 - accuracy: 0.9371\n",
      "Epoch 277/294\n",
      "109843/109843 [==============================] - 14s 127us/step - loss: 0.1936 - accuracy: 0.9387\n",
      "Epoch 278/294\n",
      "109843/109843 [==============================] - 14s 128us/step - loss: 0.1855 - accuracy: 0.9389\n",
      "Epoch 279/294\n",
      "109843/109843 [==============================] - 14s 130us/step - loss: 0.1838 - accuracy: 0.9386\n",
      "Epoch 280/294\n",
      "109843/109843 [==============================] - 14s 127us/step - loss: 0.1920 - accuracy: 0.9369\n",
      "Epoch 281/294\n",
      "109843/109843 [==============================] - 16s 150us/step - loss: 0.1880 - accuracy: 0.9391\n",
      "Epoch 282/294\n",
      "109843/109843 [==============================] - 14s 128us/step - loss: 0.1912 - accuracy: 0.9377\n",
      "Epoch 283/294\n",
      "109843/109843 [==============================] - 15s 133us/step - loss: 0.1855 - accuracy: 0.9393\n",
      "Epoch 284/294\n",
      "109843/109843 [==============================] - 14s 127us/step - loss: 0.1941 - accuracy: 0.9375\n",
      "Epoch 285/294\n",
      "109843/109843 [==============================] - 14s 131us/step - loss: 0.1908 - accuracy: 0.9378\n",
      "Epoch 286/294\n",
      "109843/109843 [==============================] - 14s 130us/step - loss: 0.1906 - accuracy: 0.9378\n",
      "Epoch 287/294\n",
      "109843/109843 [==============================] - 14s 127us/step - loss: 0.1863 - accuracy: 0.9390\n",
      "Epoch 288/294\n",
      "109843/109843 [==============================] - 15s 132us/step - loss: 0.1853 - accuracy: 0.9398\n",
      "Epoch 289/294\n",
      "109843/109843 [==============================] - 14s 129us/step - loss: 0.1942 - accuracy: 0.9377\n",
      "Epoch 290/294\n",
      "109843/109843 [==============================] - 14s 125us/step - loss: 0.1881 - accuracy: 0.9389\n",
      "Epoch 291/294\n",
      "109843/109843 [==============================] - 16s 145us/step - loss: 0.1861 - accuracy: 0.9392\n",
      "Epoch 292/294\n",
      "109843/109843 [==============================] - 14s 130us/step - loss: 0.1903 - accuracy: 0.9390\n",
      "Epoch 293/294\n",
      "109843/109843 [==============================] - 14s 128us/step - loss: 0.1942 - accuracy: 0.9380\n",
      "Epoch 294/294\n",
      "109843/109843 [==============================] - 14s 128us/step - loss: 0.1923 - accuracy: 0.9382\n",
      "\n",
      "elasped time =  0:54:14.527323\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#train + test 학습 반복\n",
    "\n",
    "\n",
    "#keras\n",
    "seed = 0 \n",
    "np.random.seed(0)\n",
    "tf.compat.v1.set_random_seed(0)\n",
    "epochs, batch_size = 294, 150 # batch -> 작을수록 좀 더 세밀한 조정 but 시간 많이걸림 \n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "modelpath=\"./test_surv_time.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "train_again = model.fit(X_total, Y_total, validation_split=0.0, epochs=epochs, \n",
    "                    batch_size=batch_size, shuffle=True, callbacks = [checkpointer])\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"\\nelasped time = \",end_time- start_time)\n",
    "print('Learning Finished!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
